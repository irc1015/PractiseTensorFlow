{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a58deb6f-31cb-4e0b-8b10-a2858eb2aa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os,math,re,sys\n",
    "#os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\" #CPU Only\n",
    "\n",
    "#os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'#GPU Running\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "print(\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f05f4dd0-6b2b-4ad2-8816-e6f162cf1c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "REPLICAS: 1\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy(devices=None) #Use all available GPUs or CPU\n",
    "print(\"REPLICAS:\", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae0fa657-d7e0-49a5-a525-8a64255eb21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = [311,311]\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 25\n",
    "\n",
    "LR_START = 0.00001\n",
    "LR_MAX = 0.00004 * strategy.num_replicas_in_sync\n",
    "LR_MIN = 0.00001\n",
    "LR_RAMPUP_EPOCHS = 3\n",
    "LR_SUSTAIN_EPOCHS = 0\n",
    "LR_EXP_DECAY = .7\n",
    "\n",
    "def scheduler_epoch(epoch):\n",
    "    if epoch < LR_RAMPUP_EPOCHS:\n",
    "        learning_rate = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
    "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
    "        learning_rate = LR_MAX\n",
    "    else:\n",
    "        learning_rate = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n",
    "    return learning_rate\n",
    "                \n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler_epoch, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cb0d39b-43cb-4470-85e8-255aa4738f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataSet_Path = '/104_Flowers'\n",
    "Train_Size_Choice = {\n",
    "    192: DataSet_Path + '/jpeg-192x192_train.tfrecord',\n",
    "    224: DataSet_Path + '/jpeg-224x224_train.tfrecord',\n",
    "    311: DataSet_Path + '/jpeg-311x311_train.tfrecord',\n",
    "    512: DataSet_Path + '/jpeg-512x512_train.tfrecord'\n",
    "}\n",
    "\n",
    "Val_Size_Choice = {\n",
    "    192: DataSet_Path + '/jpeg-192x192_val.tfrecord',\n",
    "    224: DataSet_Path + '/jpeg-224x224_val.tfrecord',\n",
    "    311: DataSet_Path + '/jpeg-311x311_val.tfrecord',\n",
    "    512: DataSet_Path + '/jpeg-512x512_val.tfrecord'\n",
    "}\n",
    "\n",
    "TrainSet_Path = Train_Size_Choice[IMG_SIZE[0]]\n",
    "ValSet_Path = Val_Size_Choice[IMG_SIZE[0]]\n",
    "\n",
    "CLASS = ['toad lily', 'love in the mist', 'monkshood', 'azalea', 'fritillary', \n",
    "         'silverbush', 'canterbury bells', 'stemless gentian', 'pink primrose', 'buttercup', \n",
    "         'poinsettia', 'desert-rose', 'bird of paradise', 'columbine', 'frangipani', \n",
    "         'sweet pea', 'siam tulip', 'great masterwort', 'hard-leaved pocket orchid', 'marigold', \n",
    "         'foxglove', 'wild pansy', 'windflower', 'daisy', 'tiger lily', \n",
    "         'purple coneflower', 'orange dahlia', 'globe-flower', 'lilac hibiscus', 'fire lily', \n",
    "         'balloon flower', 'iris', 'bishop of llandaff', 'yellow iris', 'garden phlox', \n",
    "         'alpine sea holly', 'geranium', 'pink quill', 'tree poppy', 'spear thistle', \n",
    "         'bromelia', 'common dandelion', 'sword lily', 'peruvian lily', 'carnation', \n",
    "         'cosmos', 'spring crocus', 'lotus', 'bolero deep blue', 'anthurium', \n",
    "         'rose', 'water lily', 'primula', 'blackberry lily', 'gaura', \n",
    "         'trumpet creeper', 'globe thistle', 'sweet william', 'snapdragon', 'mexican petunia', \n",
    "         'cyclamen ', 'petunia', 'gazania', 'king protea', 'blanket flower', \n",
    "         'common tulip', 'giant white arum lily', 'wild rose', 'morning glory', 'thorn apple', \n",
    "         'pincushion flower', 'tree mallow', 'canna lily', 'camellia', 'pink-yellow dahlia', \n",
    "         'bee balm', 'wild geranium', 'artichoke', 'black-eyed susan', 'ruby-lipped cattleya', \n",
    "         'clematis', 'prince of wales feathers', 'hibiscus', 'cautleya spicata', 'lenten rose', \n",
    "         'red ginger', \"colt's foot\", 'hippeastrum ', 'mallow', 'californian poppy', \n",
    "         'corn poppy', 'moon orchid', 'passion flower', 'grape hyacinth', 'japanese anemone', \n",
    "         'watercress', 'cape flower', 'osteospermum', 'barberton daisy', 'bougainvillea', \n",
    "         'magnolia', 'sunflower', 'daffodil', 'wallflower']\n",
    "\n",
    "NUM_TRAINING_IMG = 12753\n",
    "STEPS_PER_EPOCH = NUM_TRAINING_IMG // BATCH_SIZE\n",
    "NUM_VALIDATION_IMG = 3712\n",
    "VALIDATION_STEPS = -(-NUM_VALIDATION_IMG // BATCH_SIZE) #The \"-(- // )\" trick rounds up instead of down\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e738cbc-e10a-46d7-9efd-8ee46a89f511",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE_NASNET = [331,331]  \n",
    "# If you want to finetune NASNet with 'imagenet' weights, input size must be [331,331,3]\n",
    "\n",
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels=3) #JPEG --> Tensor\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    #image = tf.reshape(image, [*IMG_SIZE, 3])\n",
    "    image = tf.image.resize(image, IMG_SIZE_NASNET) #image must be 3-D, so resize it\n",
    "    return image\n",
    "\n",
    "def read_tfrecord(example):\n",
    "    image_feature_dict = {\n",
    "        'image': tf.io.FixedLenFeature([],tf.string),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image_id': tf.io.FixedLenFeature([],tf.string)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, image_feature_dict) #return a dict\n",
    "    image = decode_image(example['image'])\n",
    "    label = tf.cast(example['label'], tf.int64) #104 classes in[0,103]\n",
    "    image_id = example['image_id'] \n",
    "    return image,label   #return a tuple\n",
    "\n",
    "def load_dataset(filenames, ordered = False):\n",
    "    ignore_order = tf.data.Options()\n",
    "\n",
    "    if not ordered:\n",
    "        ignore_order.deterministic = False # experimental_deterministic has been baned\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
    "    dataset = dataset.with_options(ignore_order)\n",
    "    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n",
    "    return dataset\n",
    "\n",
    "def data_augment(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    return image, label\n",
    "\n",
    "def get_training_dataset():\n",
    "    dataset = load_dataset(TrainSet_Path) #ordered default is False\n",
    "    dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) #Prepare later elements\n",
    "    return dataset\n",
    "\n",
    "def get_validation_dataset(ordered = False):\n",
    "    dataset = load_dataset(ValSet_Path, ordered = ordered)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    #dataset = dataset.cache() #cache files in memory  Might lead to session die\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da274ff5-8afd-46d4-8ee0-450d985f8648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shapes:\n",
      "(10, 331, 331, 3) (10,)\n",
      "(10, 331, 331, 3) (10,)\n",
      "(10, 331, 331, 3) (10,)\n",
      "Training label examples: [13 68  9 22 67 63  3 50  6 44]\n",
      "13 <class 'numpy.int64'>\n",
      "[[[0.21176472 0.28235295 0.07843138]\n",
      "  [0.21176472 0.28235295 0.07843138]\n",
      "  [0.20784315 0.2784314  0.07450981]\n",
      "  ...\n",
      "  [0.18431373 0.24705884 0.0627451 ]\n",
      "  [0.18431373 0.24705884 0.0627451 ]\n",
      "  [0.18431373 0.24705884 0.0627451 ]]\n",
      "\n",
      " [[0.20784315 0.2784314  0.07450981]\n",
      "  [0.20784315 0.2784314  0.07450981]\n",
      "  [0.20784315 0.2784314  0.07450981]\n",
      "  ...\n",
      "  [0.18431373 0.24705884 0.0627451 ]\n",
      "  [0.18431373 0.24705884 0.0627451 ]\n",
      "  [0.18431373 0.24705884 0.0627451 ]]\n",
      "\n",
      " [[0.20784315 0.2784314  0.07450981]\n",
      "  [0.20784315 0.2784314  0.07450981]\n",
      "  [0.20784315 0.2784314  0.07450981]\n",
      "  ...\n",
      "  [0.18823531 0.2509804  0.06666667]\n",
      "  [0.18823531 0.2509804  0.06666667]\n",
      "  [0.18823531 0.2509804  0.06666667]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.3137255  0.45098042 0.13725491]\n",
      "  [0.3137255  0.45098042 0.13725491]\n",
      "  [0.3137255  0.45098042 0.13725491]\n",
      "  ...\n",
      "  [0.25490198 0.32156864 0.11764707]\n",
      "  [0.25490198 0.32156864 0.11764707]\n",
      "  [0.25490198 0.32156864 0.11764707]]\n",
      "\n",
      " [[0.3137255  0.45098042 0.13725491]\n",
      "  [0.3137255  0.45098042 0.13725491]\n",
      "  [0.3137255  0.45098042 0.13725491]\n",
      "  ...\n",
      "  [0.25490198 0.32156864 0.11764707]\n",
      "  [0.25490198 0.32156864 0.11764707]\n",
      "  [0.25490198 0.32156864 0.11764707]]\n",
      "\n",
      " [[0.31764707 0.4431373  0.14509805]\n",
      "  [0.31764707 0.4431373  0.14509805]\n",
      "  [0.31764707 0.4431373  0.14509805]\n",
      "  ...\n",
      "  [0.25490198 0.32156864 0.11764707]\n",
      "  [0.25490198 0.32156864 0.11764707]\n",
      "  [0.25490198 0.32156864 0.11764707]]] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data shapes:\")\n",
    "for image, label in get_training_dataset().take(3):\n",
    "    print(image.numpy().shape, label.numpy().shape)\n",
    "print(\"Training label examples:\", label.numpy())\n",
    "labelarray = label.numpy()\n",
    "print(labelarray[0],type(labelarray[0]))\n",
    "imagearray = image.numpy()\n",
    "print(imagearray[0], type(imagearray[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5698c4f0-114f-4456-b17b-37d55bd54c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lambda (Lambda)             (None, 331, 331, 3)       0         \n",
      "                                                                 \n",
      " NASNet (Functional)         (None, 11, 11, 4032)      84916818  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 4032)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " flower_prob (Dense)         (None, 104)               419432    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85,336,250\n",
      "Trainable params: 85,139,582\n",
      "Non-trainable params: 196,668\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    pretrained_model = tf.keras.applications.NASNetLarge(weights='imagenet', include_top=False)\n",
    "    pretrained_model.trainable = True # fine-tuning\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        # convert image format from int [0,255] to the format expected by this model\n",
    "        tf.keras.layers.Lambda(lambda data: tf.keras.applications.nasnet.preprocess_input(tf.cast(data, tf.float32)), input_shape=[*IMG_SIZE_NASNET, 3]),\n",
    "        pretrained_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(len(CLASS), activation='softmax', name='flower_prob')\n",
    "    ])\n",
    "    \n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss = 'sparse_categorical_crossentropy',\n",
    "    metrics=['sparse_categorical_accuracy'],\n",
    "    steps_per_execution=1\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f972c9-837e-4f82-9b83-960ee0a0d7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 1/10\n",
      "1275/1275 [==============================] - 791s 566ms/step - loss: 2.6163 - sparse_categorical_accuracy: 0.4480 - val_loss: 4.6287 - val_sparse_categorical_accuracy: 0.0110 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 2e-05.\n",
      "Epoch 2/10\n",
      "1275/1275 [==============================] - 714s 560ms/step - loss: 0.7530 - sparse_categorical_accuracy: 0.8188 - val_loss: 4.5690 - val_sparse_categorical_accuracy: 0.0248 - lr: 2.0000e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 3.0000000000000004e-05.\n",
      "Epoch 3/10\n",
      "1275/1275 [==============================] - 713s 560ms/step - loss: 0.2735 - sparse_categorical_accuracy: 0.9310 - val_loss: 4.4278 - val_sparse_categorical_accuracy: 0.0690 - lr: 3.0000e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 4e-05.\n",
      "Epoch 4/10\n",
      "1275/1275 [==============================] - 713s 559ms/step - loss: 0.1484 - sparse_categorical_accuracy: 0.9622 - val_loss: 4.4991 - val_sparse_categorical_accuracy: 0.0951 - lr: 4.0000e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 3.1e-05.\n",
      "Epoch 5/10\n",
      "1275/1275 [==============================] - 713s 559ms/step - loss: 0.0588 - sparse_categorical_accuracy: 0.9857 - val_loss: 4.6448 - val_sparse_categorical_accuracy: 0.1086 - lr: 3.1000e-05\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 2.47e-05.\n",
      "Epoch 6/10\n",
      "1275/1275 [==============================] - 712s 559ms/step - loss: 0.0322 - sparse_categorical_accuracy: 0.9929 - val_loss: 4.3289 - val_sparse_categorical_accuracy: 0.2077 - lr: 2.4700e-05\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 2.029e-05.\n",
      "Epoch 7/10\n",
      "1275/1275 [==============================] - 712s 558ms/step - loss: 0.0230 - sparse_categorical_accuracy: 0.9940 - val_loss: 3.7636 - val_sparse_categorical_accuracy: 0.2874 - lr: 2.0290e-05\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 1.7203e-05.\n",
      "Epoch 8/10\n",
      "1275/1275 [==============================] - 712s 558ms/step - loss: 0.0136 - sparse_categorical_accuracy: 0.9969 - val_loss: 2.9563 - val_sparse_categorical_accuracy: 0.4001 - lr: 1.7203e-05\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 1.50421e-05.\n",
      "Epoch 9/10\n",
      "1275/1275 [==============================] - 714s 560ms/step - loss: 0.0090 - sparse_categorical_accuracy: 0.9981 - val_loss: 2.4398 - val_sparse_categorical_accuracy: 0.5127 - lr: 1.5042e-05\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 1.3529470000000001e-05.\n",
      "Epoch 10/10\n",
      "1199/1275 [===========================>..] - ETA: 40s - loss: 0.0072 - sparse_categorical_accuracy: 0.9982"
     ]
    }
   ],
   "source": [
    "history = model.fit(get_training_dataset(), steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS,\n",
    "                    validation_data=get_validation_dataset(), validation_steps=VALIDATION_STEPS,\n",
    "                    callbacks=[lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8005f8f-47f2-488e-9fb6-7bd4fce025e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_training_curves(training, validation, title, subplot, zoom_pcent=None, ylim=None):\n",
    "    # zoom_pcent: X autoscales y axis for the last X% of data points\n",
    "    if subplot%10==1: # set up the subplots on the first call\n",
    "        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n",
    "        plt.tight_layout()\n",
    "    ax = plt.subplot(subplot)\n",
    "    ax.set_facecolor('#F8F8F8')\n",
    "    ax.plot(training)\n",
    "    ax.plot(validation, '--')\n",
    "    ax.set_title('model '+ title)\n",
    "    ax.set_ylabel(title)\n",
    "    if zoom_pcent is not None:\n",
    "        ylen = len(training)*(100-zoom_pcent)//100\n",
    "        ymin = min([min(training[ylen:]), min(validation[ylen:])])\n",
    "        ymax = max([max(training[ylen:]), max(validation[ylen:])])\n",
    "        ax.set_ylim([ymin-(ymax-ymin)/20, ymax+(ymax-ymin)/20])\n",
    "    if ylim is not None:\n",
    "        ymin = ylim[0]\n",
    "        ymax = ylim[1]\n",
    "        ax.set_ylim([ymin-(ymax-ymin)/20, ymax+(ymax-ymin)/20])\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.legend(['train', 'valid.'])\n",
    "    \n",
    "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 211, ylim=[0,2.5])\n",
    "display_training_curves(history.history['sparse_categorical_accuracy'], history.history['val_sparse_categorical_accuracy'], 'accuracy', 212)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a1b505-75ce-433d-8819-534d7bf9f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdataset = get_validation_dataset(ordered=True) # since we are splitting the dataset and iterating separately on images and labels, order matters.\n",
    "images_ds = cmdataset.map(lambda image, label: image)\n",
    "labels_ds = cmdataset.map(lambda image, label: label).unbatch()\n",
    "cm_correct_labels = next(iter(labels_ds.batch(NUM_VALIDATION_IMG))).numpy() # get everything as one batch\n",
    "cm_probabilities = model.predict(images_ds, steps=VALIDATION_STEPS)\n",
    "cm_predictions = np.argmax(cm_probabilities, axis=-1)\n",
    "print(\"Correct   labels: \", cm_correct_labels.shape, cm_correct_labels)\n",
    "print(\"Predicted labels: \", cm_predictions.shape, cm_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42f55f5-ca72-4cb6-a495-808449753369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confusion_matrix(cmat, score, precision, recall):\n",
    "    plt.figure(figsize=(25,25))\n",
    "    ax = plt.gca()\n",
    "    ax.matshow(cmat, cmap='Reds')\n",
    "    ax.set_xticks(range(len(CLASS)))\n",
    "    ax.set_xticklabels(CLASS)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n",
    "    ax.set_yticks(range(len(CLASS)))\n",
    "    ax.set_yticklabels(CLASS)\n",
    "    plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    #titlestring = \"\"\n",
    "    #if score is not None:\n",
    "    #    titlestring += 'f1 = {:.3f} '.format(score)\n",
    "    #if precision is not None:\n",
    "    #    titlestring += '\\nprecision = {:.3f} '.format(precision)\n",
    "    #if recall is not None:\n",
    "    #    titlestring += '\\nrecall = {:.3f} '.format(recall)\n",
    "    #if len(titlestring) > 0:\n",
    "    #    ax.text(101, 1, titlestring, fontdict={'fontsize': 18, 'horizontalalignment':'right', 'verticalalignment':'top', 'color':'#804040'})\n",
    "    plt.show()\n",
    "    \n",
    "cmat = confusion_matrix(cm_correct_labels, cm_predictions, labels=range(len(CLASS)))\n",
    "score = f1_score(cm_correct_labels, cm_predictions, labels=range(len(CLASS)), average='macro')\n",
    "precision = precision_score(cm_correct_labels, cm_predictions, labels=range(len(CLASS)), average='macro')\n",
    "recall = recall_score(cm_correct_labels, cm_predictions, labels=range(len(CLASS)), average='macro')\n",
    "cmat = (cmat.T / cmat.sum(axis=1)).T # normalized\n",
    "display_confusion_matrix(cmat, score, precision, recall)\n",
    "print('f1 score: {:.3f}, precision: {:.3f}, recall: {:.3f}'.format(score, precision, recall))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
